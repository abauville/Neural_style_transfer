{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"Blog02_Style.ipynb","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"b1841aa60c194bbbbd63caecf02e1fb9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_989270f4222e4c24b6195b0e2c6f28ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ad4a63e3cb94466d92f770b9c26181b1","IPY_MODEL_f5f1126fb0ad4d7c8286a4933ee0fe87"]}},"989270f4222e4c24b6195b0e2c6f28ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ad4a63e3cb94466d92f770b9c26181b1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4a197f1032c243ac8534a51fe3882ade","_dom_classes":[],"description":" 42%","_model_name":"FloatProgressModel","bar_style":"","max":553433881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231317504,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9dfc7f3127ee42579e748876d03ea35f"}},"f5f1126fb0ad4d7c8286a4933ee0fe87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_381a3ba7fc5d4f36b122e8188be0e2bf","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 221M/528M [00:02&lt;00:02, 108MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ceb451498171483199d83d3b64cd29f8"}},"4a197f1032c243ac8534a51fe3882ade":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9dfc7f3127ee42579e748876d03ea35f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"381a3ba7fc5d4f36b122e8188be0e2bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ceb451498171483199d83d3b64cd29f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"35abe7f9-09b0-4b91-8378-3a63dd1b66da"},"source":["- title: \"Neural style transfer 2: style\"\n","- description: \"Feature visualization with PyTorch\"\n","- toc: false\n","- branch: master\n","- badges: true\n","- comments: true\n","- categories: [fastpages, jupyter]\n","- image: images/some_folder/your_image.png\n","- hide: false\n","- search_exclude: true\n","- metadata_key1: metadata_value1\n","- metadata_key2: metadata_value2"],"id":"35abe7f9-09b0-4b91-8378-3a63dd1b66da"},{"cell_type":"code","metadata":{"id":"878aa2c5-0e9b-4949-b5a8-ba7c91cbcb3e","executionInfo":{"status":"ok","timestamp":1625123647560,"user_tz":-540,"elapsed":4782,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}}},"source":["# Imports\n","import torch\n","import torchvision\n","from torch import nn\n","import skimage\n","from skimage import transform\n","from skimage import io\n","# from im_func import show_image, timer\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import display, clear_output\n","import torch.nn.functional as F\n","import torchvision.transforms.functional as TF\n","from time import time\n","import contextlib\n","\n","@contextlib.contextmanager\n","def timer(msg='timer'):\n","    tic = time()\n","    yield\n","    return print(f\"{msg}: {time() - tic:.2f}\")"],"id":"878aa2c5-0e9b-4949-b5a8-ba7c91cbcb3e","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4c22e8f-37a3-4f52-9f8e-e0701e6863c2","executionInfo":{"status":"ok","timestamp":1625123647561,"user_tz":-540,"elapsed":5,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}}},"source":["rgb_mean = torch.tensor([0.485, 0.456, 0.406]) # Fixed values for PyTorch pretrained models\n","rgb_std = torch.tensor([0.229, 0.224, 0.225])\n","\n","class Image(nn.Module):\n","    def __init__(self, img=None, optimizable=True, img_shape=[64,64], jit_max=2, angle_max=2.0):\n","        super(Image,self).__init__()\n","        \n","        self.img_shape = img_shape\n","        \n","        if type(img)==type(None):\n","            self.img = torch.randn([1, 3] + self.img_shape)\n","        else:\n","            self.img = img\n","            self.img = self.preprocess()\n","\n","        if optimizable == True:\n","            self.img = nn.Parameter(self.img)\n","         \n","        self.jit_i = 0\n","        self.jit_j = 0\n","        self.jit_max = jit_max\n","        self.angle = 0.0\n","        self.angle_max = angle_max\n","    def preprocess(self):\n","        with torch.no_grad():\n","            transforms = torchvision.transforms.Compose([\n","                torchvision.transforms.ToPILImage(),\n","                torchvision.transforms.Resize(self.img_shape),\n","                torchvision.transforms.ToTensor(),\n","            ])\n","            return transforms(self.img).unsqueeze(0)\n","            \n","\n","    def postprocess(self):\n","        with torch.no_grad():\n","            img = self.img.data[0].to(rgb_std.device).clone()\n","        return torchvision.transforms.ToPILImage()(img.permute(1, 2, 0).permute(2, 0, 1))\n","          \n","    def jittered_image(self):\n","        with torch.no_grad():\n","            jit_max = 2\n","            temp = np.random.standard_normal(2)*2.0\n","            self.jit_i += temp[0]\n","            self.jit_j += temp[1]\n","\n","            self.angle += np.random.standard_normal(1)[0]*1.0\n","            self.angle = np.clip(self.angle,-self.angle_max,self.angle_max)\n","            self.jit_i, self.jit_j = np.clip([self.jit_i, self.jit_j],-self.jit_max,self.jit_max)#.astype(int)\n","            print(self.angle, self.jit_i, self.jit_j, temp)\n","            return torchvision.transforms.functional.affine(self.img.data, angle=self.angle, translate=(self.jit_i/self.img_shape[1], self.jit_j/self.img_shape[0]), scale=1., shear=[0.0,0.0])#,interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n","            \n","        \n","    def forward(self, jitter=False):\n","        if jitter:\n","            return self.jittered_image()\n","        else:\n","            return self.img\n","            "],"id":"e4c22e8f-37a3-4f52-9f8e-e0701e6863c2","execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"94d2ec43-9d5a-4c6c-8763-389e1d1886fd","outputId":"195c804c-5b38-43ad-f38b-1e6a3df16ee1","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["b1841aa60c194bbbbd63caecf02e1fb9","989270f4222e4c24b6195b0e2c6f28ef","ad4a63e3cb94466d92f770b9c26181b1","f5f1126fb0ad4d7c8286a4933ee0fe87","4a197f1032c243ac8534a51fe3882ade","9dfc7f3127ee42579e748876d03ea35f","381a3ba7fc5d4f36b122e8188be0e2bf","ceb451498171483199d83d3b64cd29f8"]}},"source":["pretrained_net = torchvision.models.vgg16(pretrained=True)#.features.to(device).eval()\n","display(pretrained_net.features)\n","content_layer = [7]"],"id":"94d2ec43-9d5a-4c6c-8763-389e1d1886fd","execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1841aa60c194bbbbd63caecf02e1fb9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"d4d6fc89-75f1-4300-8691-1daca508396a"},"source":["class SmallNet(nn.Module):\n","    def __init__(self, pretrained_net, last_layer):\n","        super(SmallNet,self).__init__()\n","        self.net= nn.Sequential(*([torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)] + \n","                     [pretrained_net.features[i]\n","                            for i in range(last_layer + 1)])).to(device).eval()\n","\n","    def forward(self, X, extract_layers, gram=False):\n","        # Passes the image X through the pretrained network and \n","        # returns a list containing the feature maps of layers specified in the list extract_layers\n","        detach = not(X.requires_grad) # We don't want to keep track of the gradients on the content image\n","        feature_maps = []\n","        for il in range(len(self.net)):\n","            X = self.net[il](X)\n","            if (il-1 in extract_layers): # note: il-1 because I added a normalization layer before the pretrained net in self.net\n","                if detach:\n","                    feature_maps.append(X.clone().detach())    \n","                else:\n","                    feature_maps.append(X.clone())\n","                    \n","        if gram:\n","            return self._get_gram(feature_maps)\n","        else:\n","            return feature_maps\n","    \n","    def _get_gram(self, feature_maps):\n","        gram_matrices = []\n","        for fm in feature_maps:\n","            a, b, c, d = fm.size()  # a=batch size(=1)\n","            features = fm.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n","            G = torch.mm(features, features.t())  # compute the gram product\n","            gram_matrices.append(G.div(a * b * c * d))\n","        return gram_matrices"],"id":"d4d6fc89-75f1-4300-8691-1daca508396a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"49346faf-00f5-4494-94c4-46e461f8125c"},"source":["class Losses(nn.Module):\n","    def __init__(self, img_ref, \n","                 content_weight=1.0, tv_weight_ini=0.0, clamp_weight_ini=1.0, int_weight_ini=0.0, \n","                 alpha=6, beta=1.5,\n","                 weight_adjust_bounds=[0.01,0.1],\n","                 weight_adjust_fac=[1.5, 0.75]):\n","        super(Losses,self).__init__()\n","        # img_ref is used to compute a reference total variation and reference intensity\n","        # tv_weight: weight of the total variation regularizer\n","        # int_weight: weight of the intensity regularizer\n","        # alpha: exponent for the intensity regularizer\n","        # beta: exponent for the total variation regularizer\n","        self.content_weight = content_weight\n","        self.tv_weight = tv_weight_ini\n","        self.int_weight = int_weight_ini\n","        self.clamp_weight = clamp_weight_ini\n","        self.content_loss = 0.0\n","        self.tv_loss = 0.0\n","        self.int_loss = 0.0\n","        self.total_loss = 0.0\n","        \n","        self.alpha = alpha\n","        self.beta = beta\n","        \n","        self.B, self.V = self.get_regularizer_refs(img_ref)\n","        \n","        self.weight_adjust_bounds = weight_adjust_bounds\n","        self.weight_adjust_fac = weight_adjust_fac\n","        \n","\n","    def get_content_loss(self, Y_hat, Y, reduction='mean'):\n","        # Mean squared error between generated and content image\n","        loss = 0\n","#         for i in range(len(feature_map_content)):\n","#             loss += torch.mean((feature_map_content[i]-feature_map_gen[i])**2)\n","#         return loss\n","        for y_hat, y in zip(Y_hat,Y):\n","            loss += F.mse_loss(y_hat, y.detach(),reduction=reduction)\n","        return loss\n","    \n","    \n","    def get_style_loss(gram_style_image, gram_generated_image):\n","        loss = 0\n","        for A, G in zip(gram_style_image,gram_generated_image):\n","            loss += F.mse_loss(G, A.detach(),reduction='sum')\n","        return loss\n","    \n","    def get_regularizer_refs(self, img):\n","        eps = 1e-10\n","        L2 = torch.sqrt(img[:,0,:,:]**2 + img[:,1,:,:]**2 + img[:,2,:,:]**2 + eps)\n","        B = L2.mean()\n","\n","        d_dx = img[:,:,1:,:]-img[:,:,:-1,:]\n","        d_dy = img[:,:,:,1:]-img[:,:,:,:-1]\n","        L2 = torch.sqrt(d_dx[:,:,:,1:]**2 + d_dy[:,:,1:,:]**2 + eps)\n","        V = L2.mean()\n","        return B, V\n","\n","    def get_int_loss(self, img):\n","        # Intensity loss\n","        H = img.shape[2]\n","        W = img.shape[3]\n","        eps = 1e-10\n","        L2 = torch.sqrt(img[:,0,:,:]**2 + img[:,1,:,:]**2 + img[:,2,:,:]**2 + eps)\n","        \n","        loss = 1./H/W/(self.B**self.alpha) * torch.sum(L2**self.alpha)\n","        \n","        return loss\n","\n","    def get_clamp_loss(self, img):\n","#         loss = torch.sum(img[img>1.0]**2) + torch.sum((1.0-img[img<0.0])**2)\n","        H = img.shape[2]\n","        W = img.shape[3]\n","        loss = 1.0/H/W * (torch.sum(torch.abs(img[img>1.0]-1.0)**2) + torch.sum(torch.abs(img[img<0.0])**2))\n","        return loss\n","\n","    def get_TV_loss(self, img):\n","        # Total variation loss\n","        H = img.shape[2]\n","        W = img.shape[3]\n","        C = img.shape[1]\n","        eps = 1e-10 # avoids accidentally taking the sqrt of a negative number because of rounding errors\n","\n","        # # total variation\n","        d_dx = img[:,:,1:,:]-img[:,:,:-1,:]\n","        d_dy = img[:,:,:,1:]-img[:,:,:,:-1]\n","        # I ignore the first row or column of the image when computing the norm, in order to have vectors with matching sizes\n","        # Thus, d_dx and d_dy are not strictly colocated, but that should be a good enough approximation because neighbouring pixels are correlated\n","        L2 = torch.sqrt(d_dx[:,:,:,1:]**2 + d_dy[:,:,1:,:]**2 + eps)\n","        TV = torch.sum(L2**self.beta) # intensity regularizer\n","\n","        loss = 1./H/W/(self.V**self.beta) * TV\n","#         loss = 1./H/W * (torch.sum(d_dx**2) + torch.sum(d_dy**2))\n","        return loss\n","    \n","    def _adjust_weight(self,weight,loss):\n","        lb, ub = self.weight_adjust_bounds\n","        lfac, ufac = self.weight_adjust_fac\n","        if weight>1e-10: # if weight_ini=0, the user wants to switch the weight off, we don't want to accidentally activate it because of rounding errors\n","            if weight*loss<lb*(self.content_weight*self.content_loss):\n","                weight *= lfac\n","            if weight*loss>ub*(self.content_weight*self.content_loss):\n","                weight *= ufac\n","        return weight\n","                \n","    def adjust_weights(self):\n","        \n","        self.tv_weight = self._adjust_weight(self.tv_weight,self.tv_loss)\n","        self.int_weight = self._adjust_weight(self.int_weight,self.int_loss)\n","          \n","        if self.clamp_loss>1e-10:\n","            self.clamp_weight = self._adjust_weight(self.clamp_weight,self.clamp_loss)\n","    \n","    def forward(self,img,feature_map, feature_map_target):\n","#         self.content_loss = self.get_content_loss(feature_map, feature_map_target)\n","        self.content_loss = self.get_content_loss(feature_map, feature_map_target,reduction='sum')\n","        self.int_loss = self.get_int_loss(img)\n","        self.tv_loss = self.get_int_loss(img)\n","        self.clamp_loss = self.get_clamp_loss(img)\n","        \n","        self.total_loss = ( self.content_weight*self.content_loss \n","                    + self.int_weight*self.int_loss \n","                    + self.tv_weight*self.tv_loss \n","                    + self.clamp_weight*self.clamp_loss )\n","        \n","        return self.total_loss"],"id":"49346faf-00f5-4494-94c4-46e461f8125c","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1bbaa1ef-6137-49f5-b0c3-c5b179695e86"},"source":["device = 'cpu'\n","\n","# Images\n","# content_im = skimage.io.imread(\"https://github.com/scijs/baboon-image/blob/master/baboon.png?raw=true\")\n","content_im = skimage.io.imread(\"https://github.com/abauville/Neural_style_transfer/blob/main/Data/matisse_cat.jpeg?raw=true\")\n","fig, ax = plt.subplots(1,1,figsize=[5,5])\n","_ = plt.imshow(content_im); plt.title(\"content\"); _ = plt.axis(\"off\")\n","\n","img_content = Image(img=content_im, optimizable=False).to(device)\n","img_gen = Image(None, optimizable=True).to(device)\n","\n","# SmallNet\n","net = SmallNet(pretrained_net, content_layer[-1])\n","\n","# Losses\n","loss_fn = Losses(img_content(), \n","                 tv_weight_ini=1e3, \n","                 int_weight_ini=0.0,\n","                 clamp_weight_ini=1e1,\n","                 weight_adjust_bounds=[0.01, 0.1],\n","                 weight_adjust_fac=[1.5, 0.75])\n","\n","# Optimizer\n","optimizer = torch.optim.LBFGS(img_gen.parameters(),lr=1.0)\n","abs_loss_limit = 1e-3\n","rel_loss_limit = 1e-7\n","\n","# Jitter the input image?\n","jitter_nsteps = 30"],"id":"1bbaa1ef-6137-49f5-b0c3-c5b179695e86","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05038f8a-9d12-4d81-9df1-dca0b8be45d1"},"source":["fig, ax = plt.subplots(1,1,figsize=[10,10])\n","\n","# for sanity\n","if jitter_nsteps<0:\n","    jitter_nsteps = 0\n","\n","def closure():\n","    optimizer.zero_grad()\n","    fm_gen = net(img_gen(), content_layer, gram=True)\n","    loss = loss_fn(img_gen(), fm_gen, fm_content)\n","    loss.backward()\n","    return loss\n","\n","last_loss = 1e10\n","frame = 0\n","for i in range(1000):\n","    if i<jitter_nsteps:\n","        fm_content = net(img_content(jitter=True), content_layer, gram=True)\n","    elif i==jitter_nsteps:\n","        fm_content = net(img_content(jitter=False), content_layer, gram=True)\n","    optimizer.step(closure)\n","    loss_fn.adjust_weights()\n","\n","    if i%1==0:\n","        with torch.no_grad():\n","            plt.clf()\n","            plt.imshow(img_gen.postprocess())\n","            \n","            plt.title(f\"epoch {i:02}, content, tv, intensity, clamp losses:\" + \n","                      f\"{loss_fn.content_weight*loss_fn.content_loss:.2e}, \" + \n","                      f\"{loss_fn.tv_weight*loss_fn.tv_loss:.2e}, \" +\n","                      f\"{loss_fn.int_weight*loss_fn.int_loss:.2e}, \" + \n","                      f\"{loss_fn.clamp_weight*loss_fn.clamp_loss:.2e}, \"+\n","                      f\"total:{loss_fn.total_loss:.2e}, abs: {torch.abs(last_loss-loss_fn.total_loss):.2e}\")\n","            \n","            clear_output(wait = True)\n","            display(fig)\n","            \n","            # plt.savefig(f\"./Output/Frame{frame:05d}\")\n","            # frame += 1\n","\n","            if loss_fn.total_loss<abs_loss_limit and loss_fn.clamp_loss<1e-10:\n","                clear_output(wait = True)\n","                print(f'success: absolute loss limit ({abs_loss_limit:.1e}) reached')\n","                break\n","            if torch.abs(last_loss-loss_fn.total_loss)<rel_loss_limit and loss_fn.clamp_loss<1e-10:\n","                clear_output(wait = True)\n","                print(f'stopped because relative loss limit ({rel_loss_limit:.1e})  was reached')\n","                break\n","\n","            if loss_fn.total_loss.isnan():\n","                print(f'stopped because loss is NaN')\n","                break\n","                \n","    last_loss = loss_fn.total_loss\n","\n","    with torch.no_grad():\n","        img_gen.img.data.clamp_(0,1)"],"id":"05038f8a-9d12-4d81-9df1-dca0b8be45d1","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"15a70f75-91b6-4626-a89a-c53c206b0b69"},"source":[""],"id":"15a70f75-91b6-4626-a89a-c53c206b0b69","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7153685a-4509-40bb-8947-ab38c5f672e5"},"source":[""],"id":"7153685a-4509-40bb-8947-ab38c5f672e5","execution_count":null,"outputs":[]}]}
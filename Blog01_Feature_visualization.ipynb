{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1009e6da",
   "metadata": {
    "id": "1009e6da"
   },
   "source": [
    "\n",
    "- title: \"Neural style transfer 1: feature visualization\"\n",
    "- description: \"Feature visualization with PyTorch\"\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [fastpages, jupyter]\n",
    "- image: images/some_folder/your_image.png\n",
    "- hide: false\n",
    "- search_exclude: true\n",
    "- metadata_key1: metadata_value1\n",
    "- metadata_key2: metadata_value2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d453185",
   "metadata": {
    "id": "8d453185"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Neural style transfer\n",
    "\n",
    "This is the first part of a three-part series on *neural style transfer*. Neural style transfer is the application of deep learning to the task of texture transfer. The goal is to transfer some characteristics of a *style image* (e.g. a painting) onto a *content image*. The technique can be used for artistic purpose or to enhance the *content image* with missing information, e.g. in super-resolution applications. \n",
    "\n",
    "**NOTE: here insert an illustration**\n",
    "\n",
    "\n",
    "In this series I will mostly follow the original implementation by [Gatys et al. (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf), and complement it with several \n",
    "improvements suggested in the literature. In Gatys et al.'s algorithm the *content* and *style* images are passed through a pretrained convolutional neural network (e.g. VGG19). The feature maps that result from the various convolution operations contain increasingly abstract representations of the original images. Thus, working with feature maps rather the the original image offers a more flexible way of combining them. From these feature maps, the algorithm performs three steps:\n",
    "\n",
    "1. feature visualization\n",
    "2. style extraction\n",
    "3. merging the *content image* with the style of the *style image*.\n",
    "\n",
    "In this post, we focus on the process of feature visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lUIVIgZogWyZ",
   "metadata": {
    "id": "lUIVIgZogWyZ"
   },
   "source": [
    "## Feature visualization\n",
    "\n",
    "Feature visualization refers to an ensemle of techniques employed to extract, visualize or understand the information (weights, bias, feature maps) inside a neural network. [Olah et al. (2017)](https://distill.pub/2017/feature-visualization/) provide a good introduction to the subject, and the [openAI microscope](https://microscope.openai.com/models) allows one to explore pretrained convolutional networks through feature visualization.\n",
    "\n",
    "Neural style transfer relies on the technique of **inversion** (see Mahendran and Vedaldi [2014](https://arxiv.org/pdf/1512.02017.pdf), [2016](https://arxiv.org/abs/1512.02017)). \"*We do so by modelling a representation as a\n",
    "function $\\phi_0 = \\phi(x_0)$ of the image $x_0$. Then, we attempt to recover the image from the information contained only in the code $\\phi_0$*\" (Mahendran and Vedaldi, 2016). Because some operation such as RELu or pooling operations (taking the max or mean of a subset of pixel) are destructive the image $x_0$ is not uniquely recoverable. For the same reason, it is easier to invert images from lower layers of the network than from higher ones. A couple of techniques such as total variation regularization or jittering the input image can help overcoming these theoretical limitations (see [Mordvintsev et al., 2016](https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html) and previous references)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d719c59-0231-4877-8807-e34c399455e3",
   "metadata": {},
   "source": [
    "## Algorithm outline\n",
    "\n",
    "In this blogpost, I implement feature inversion as described in [Mahendran and Vedaldi (2016)](https://arxiv.org/abs/1512.02017). By doing so, I also lay the core foundation for the whole neural style transfer algorithm that I'll expand the next part of this series. The algorithm is implemented in PyTorch and I used the implementations from the [D2L book](http://d2l.ai/chapter_computer-vision/neural-style.html) and this [PyTorch tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) as inspiration and benchmark.\n",
    "\n",
    "We have two images of interest: the *content image* (input) and the *generated image* (output). The algorithm and implementation goes as follows:\n",
    "1. We import the necessary libraries\n",
    "1. We create a `Image` class to store and transform images.\n",
    "1. We download a pretrained netowrk and create a smaller neural network, `SmallNet` that implements only the lower layers that we need. That network takes an image as input and outputs the feature maps of requested layers.\n",
    "1. We define a loss function between the *content* and *generated* feature maps, as well as a series of regularizers to limit inversion artefacts.\n",
    "1. Finally, we instantiate all the classes and train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c1694",
   "metadata": {
    "id": "211c1694"
   },
   "source": [
    "Notes:\n",
    "\n",
    "REF on neural style transfer:\n",
    "\n",
    "- https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf\n",
    "- https://d2l.ai/chapter_computer-vision/neural-style.html\n",
    "\n",
    "REF on feature vis:\n",
    "- https://arxiv.org/abs/1412.0035\n",
    "- https://arxiv.org/pdf/1512.02017.pdf\n",
    "- https://distill.pub/2017/feature-visualization/\n",
    "- https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html\n",
    "\n",
    "REF on texture synthesis:\n",
    "- https://arxiv.org/pdf/1606.01286.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd82f1",
   "metadata": {
    "id": "3abd82f1"
   },
   "source": [
    "# 1. Setup\n",
    "\n",
    "We start by importing all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1df70ee",
   "metadata": {
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1624860275841,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "c1df70ee"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from skimage import io\n",
    "# from im_func import show_image, timer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from time import time\n",
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer(msg='timer'):\n",
    "    tic = time()\n",
    "    yield\n",
    "    return print(f\"{msg}: {time() - tic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b1300-ebac-4769-bdf9-1f261685b30a",
   "metadata": {},
   "source": [
    "# 2. `Image` class\n",
    "\n",
    "Next we define an `Image` class. This class stores the image. It also applies preprocessing when the class is instantiated. I also added a function to perform image jittering, either translating or rotating the image. This kind of transformation, when applied to the input (i.e. content) image has been shown to improve feature inversion, especially when attempting to recover higher layers in the network. In effect those transformations enforce correlation between neighbouring pixels and help recover the information that was lost during pooling operations ([Mahendran and Vedaldi, 2016](https://arxiv.org/abs/1512.02017)). However, making the image jump all over the place from one iteration of the solver to the other may render optimization difficult or unstable. Thus, I opted to implement jittering a a random walk of the translation distance and angle of rotation. During instanciation we will set the parameter `optimizable` to `False` for the *content image*, and `True` for the *generated image*. The image is cast into a `tensor` when `optimizable==False`, and into a `nn.Parameter` with `requires_grad==True` object when `optimizable==True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93075000-0407-4a30-b3a1-1a30a41d18bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406]) # Fixed values for PyTorch pretrained models\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "class Image(nn.Module):\n",
    "    def __init__(self, img=None, optimizable=True, img_shape=[64,64], jit_max=2, angle_max=2.0):\n",
    "        super(Image,self).__init__()\n",
    "        \n",
    "        self.img_shape = img_shape\n",
    "        \n",
    "        if type(img)==type(None):\n",
    "            self.img = torch.randn([1, 3] + self.img_shape)\n",
    "        else:\n",
    "            self.img = img\n",
    "            self.img = self.preprocess()\n",
    "\n",
    "        if optimizable == True:\n",
    "            self.img = nn.Parameter(self.img)\n",
    "         \n",
    "        self.jit_i = 0\n",
    "        self.jit_j = 0\n",
    "        self.jit_max = jit_max\n",
    "        self.angle = 0.0\n",
    "        self.angle_max = angle_max\n",
    "    def preprocess(self):\n",
    "        with torch.no_grad():\n",
    "            transforms = torchvision.transforms.Compose([\n",
    "                torchvision.transforms.ToPILImage(),\n",
    "                torchvision.transforms.Resize(self.img_shape),\n",
    "                torchvision.transforms.ToTensor(),\n",
    "            ])\n",
    "            return transforms(self.img).unsqueeze(0)\n",
    "            \n",
    "\n",
    "    def postprocess(self):\n",
    "        with torch.no_grad():\n",
    "            img = self.img.data[0].to(rgb_std.device).clone()\n",
    "        return torchvision.transforms.ToPILImage()(img.permute(1, 2, 0).permute(2, 0, 1))\n",
    "          \n",
    "    def jittered_image(self):\n",
    "        with torch.no_grad():\n",
    "            jit_max = 2\n",
    "            temp = np.random.standard_normal(2)*2.0\n",
    "            self.jit_i += temp[0]\n",
    "            self.jit_j += temp[1]\n",
    "\n",
    "            self.angle += np.random.standard_normal(1)[0]*1.0\n",
    "            self.angle = np.clip(self.angle,-self.angle_max,self.angle_max)\n",
    "            self.jit_i, self.jit_j = np.clip([self.jit_i, self.jit_j],-self.jit_max,self.jit_max)#.astype(int)\n",
    "            print(self.angle, self.jit_i, self.jit_j, temp)\n",
    "            return torchvision.transforms.functional.affine(self.img.data, angle=self.angle, translate=(self.jit_i/self.img_shape[1], self.jit_j/self.img_shape[0]), scale=1., shear=[0.0,0.0])#,interpolation=torchvision.transforms.functional.InterpolationMode.BILINEAR)\n",
    "            \n",
    "        \n",
    "    def forward(self, jitter=False):\n",
    "        if jitter:\n",
    "            return self.jittered_image()\n",
    "        else:\n",
    "            return self.img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe564f",
   "metadata": {
    "id": "4ffe564f"
   },
   "source": [
    "# 3. `SmallNet` class\n",
    "\n",
    "Next, we import a pretrained model from the PyTorch zoo. Here, I use VGG16 ([Simonyan and Zisserman, 2014](https://arxiv.org/abs/1409.1556)). The VGG architecture (printed below) is composed of a series of of blocks. The smallest block unit is composed of a convolutional layer + ReLU activation layer (**Conv2d+ReLU**). Larger blocks are composed of a series of **Conv2d+ReLU** followed by a maximum pooling (**MaxPool2d**) operation. The pooling operation divides the number of pixels by two. The complete VGG16 architecture also contains a final few fully connected layers to perform the classification, but we don't need those here. I'll extract features from layer 7, following [Johnson et al. (2016)](https://arxiv.org/abs/1603.08155)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a76b5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644,
     "referenced_widgets": [
      "c4ae12751d764e319493100bd1b263c4",
      "795cf662825d4b69b89fdd2c13659236",
      "7cadfc3f543c47afb5feecdfbd9d3228",
      "4352c9184ab94a81904134f8f0b14af0",
      "00d07475ef754aa9bd2ee65d2297e211",
      "0e53203d08df409d879e56596de0da32",
      "d15b9e21a5f94415b6f5b5a14d3e4c8a",
      "401dc2fa55cc44cbb7afe2558e1c50c6"
     ]
    },
    "executionInfo": {
     "elapsed": 6728,
     "status": "ok",
     "timestamp": 1624860293272,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "27a76b5e",
    "outputId": "84146e0d-5733-4f03-da5c-1b70c0812a16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pretrained_net = torchvision.models.vgg16(pretrained=True)#.features.to(device).eval()\n",
    "display(pretrained_net.features)\n",
    "content_layer = [7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce160c4",
   "metadata": {
    "id": "9ce160c4"
   },
   "source": [
    "We are only interested in the features contains in the list of layers defined earlier in `content_layer`. Thus, we don't need to make a complete forward pass through the model. We need to feed our image to the VGG16 network up to the last layer of `content_layer` only. Thus, we create a small network class `SmallNet` that is a smaller version of the VGG16 network. We also add an initial normalization operation before the VGG16 network. We include the normalization in the network rather than as preprocessing to be able to clamp the optimized image later (a trick I got from [there](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)).\n",
    "\n",
    "A forward pass through `SmallNet` outputs a list of feature maps for specified layers (here we would specify the `content_layer` list). We will use `SmallNet` both on the *content* and *generated* image to obtain their respective feature maps. Here there is subtlety: the *content* image is not optimizable (i.e. `requires_grad==False`), but the *generated* image is optimizable (i.e. `requires_grad==False`). Thus, we need to *detach* the content image's feature map to avoid tracking their gradient during optimization. We also need to make copies of the feature maps to be used later during optimization. This part is essential but a bit tricky. When I implemented the algorithm, first, I didn't `clone()` the layers and backpropagation was crashing, but the error message is not very explicit. Then, I didn't detach the content feature maps. The backpropagation was not crashing and I was getting a reasonable output, but convergence was terrible and the results were quite underwhelming. It took me a while to figure out the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e81bfff",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1624861788104,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "2e81bfff"
   },
   "outputs": [],
   "source": [
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, pretrained_net, last_layer):\n",
    "        super(SmallNet,self).__init__()\n",
    "        self.net= nn.Sequential(*([torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)] + \n",
    "                     [pretrained_net.features[i]\n",
    "                            for i in range(last_layer + 1)])).to(device).eval()\n",
    "\n",
    "    def forward(self, X, extract_layers):\n",
    "        # Passes the image X through the pretrained network and \n",
    "        # returns a list containing the feature maps of layers specified in the list extract_layers\n",
    "        detach = not(X.requires_grad) # We don't want to keep track of the gradients on the content image\n",
    "        feature_maps = []\n",
    "        for il in range(len(self.net)):\n",
    "            X = self.net[il](X)\n",
    "            if (il-1 in extract_layers): # note: il-1 because I added a normalization layer before the pretrained net in self.net\n",
    "                if detach:\n",
    "                    feature_maps.append(X.clone().detach())    \n",
    "                else:\n",
    "                    feature_maps.append(X.clone())\n",
    "                    \n",
    "        return feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54399e56",
   "metadata": {
    "id": "54399e56"
   },
   "source": [
    "# 4. `Losses` class\n",
    "\n",
    "The *content loss* is the mean squared error between the feature maps that correspond to the *content* and *generated* images. A number of artefacts commonly appear during image inversion. One of the main reason for these artefacts is that some of the information from the original image is destroyed by pooling operations, and to a lesser extent by convolution and ReLU operations. To decrease the effect of these artefacts, we add a number of regularizer to the loss function. The *intensity regularizer* discourages large color intensity. The *total variation regularizer* inhibits small wavelength variation (i.e. denoising). Mahendran and Vedaldi (2016) and this [PyTorch tutorial](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) propose to clamp the values of the generated that are >1 or <0. I found that, in practice hard clamping the values made the optimization unstable. Instead, I created a smooth loss function that is 0 when the 0<value<1, and increases with the squared distance from 1 or 0 otherwise. I found that this regularizer achieves the objective of clamping the values while keeping the optimization stable. Each regularization term is associated with a weight. The losses and regularizers are described in detail in [Mahendran and Vedaldi (2016)](https://arxiv.org/abs/1512.02017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441900ad",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1624862285524,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "441900ad"
   },
   "outputs": [],
   "source": [
    "class Losses(nn.Module):\n",
    "    def __init__(self, img_ref, content_weight=1.0, tv_weight=0.0, clamp_weight=1.0, int_weight=0.0, alpha=6, beta=1.5):\n",
    "        super(Losses,self).__init__()\n",
    "        # img_ref is used to compute a reference total variation and reference intensity\n",
    "        # tv_weight: weight of the total variation regularizer\n",
    "        # int_weight: weight of the intensity regularizer\n",
    "        # alpha: exponent for the intensity regularizer\n",
    "        # beta: exponent for the total variation regularizer\n",
    "        self.content_weight = content_weight\n",
    "        self.tv_weight = tv_weight\n",
    "        self.int_weight = int_weight\n",
    "        self.clamp_weight = clamp_weight\n",
    "        self.content_loss = 0.0\n",
    "        self.tv_loss = 0.0\n",
    "        self.int_loss = 0.0\n",
    "        self.total_loss = 0.0\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.B, self.V = self.get_regularizer_refs(img_ref)\n",
    "        \n",
    "\n",
    "    def get_content_loss(self, feature_map_gen, feature_map_content):\n",
    "        # Mean squared error between generated and content image\n",
    "        loss = 0\n",
    "        for i in range(len(feature_map_content)):\n",
    "            loss += torch.mean((feature_map_content[i]-feature_map_gen[i])**2)\n",
    "        return loss\n",
    "\n",
    "    def get_regularizer_refs(self, img):\n",
    "        eps = 1e-10\n",
    "        L2 = torch.sqrt(img[:,0,:,:]**2 + img[:,1,:,:]**2 + img[:,2,:,:]**2 + eps)\n",
    "        B = L2.mean()\n",
    "\n",
    "        d_dx = img[:,:,1:,:]-img[:,:,:-1,:]\n",
    "        d_dy = img[:,:,:,1:]-img[:,:,:,:-1]\n",
    "        L2 = torch.sqrt(d_dx[:,:,:,1:]**2 + d_dy[:,:,1:,:]**2 + eps)\n",
    "        V = L2.mean()\n",
    "        return B, V\n",
    "\n",
    "    def get_int_loss(self, img):\n",
    "        # Intensity loss\n",
    "        H = img.shape[2]\n",
    "        W = img.shape[3]\n",
    "        eps = 1e-10\n",
    "        L2 = torch.sqrt(img[:,0,:,:]**2 + img[:,1,:,:]**2 + img[:,2,:,:]**2 + eps)\n",
    "        \n",
    "        loss = 1./H/W/(self.B**self.alpha) * torch.sum(L2**self.alpha)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_clamp_loss(self, img):\n",
    "#         loss = torch.sum(img[img>1.0]**2) + torch.sum((1.0-img[img<0.0])**2)\n",
    "        H = img.shape[2]\n",
    "        W = img.shape[3]\n",
    "        loss = 1.0/H/W * (torch.sum(torch.abs(img[img>1.0]-1.0)**2) + torch.sum(torch.abs(img[img<0.0])**2))\n",
    "        return loss\n",
    "\n",
    "    def get_TV_loss(self, img):\n",
    "        # Total variation loss\n",
    "        H = img.shape[2]\n",
    "        W = img.shape[3]\n",
    "        C = img.shape[1]\n",
    "        eps = 1e-10 # avoids accidentally taking the sqrt of a negative number because of rounding errors\n",
    "\n",
    "        # # total variation\n",
    "        d_dx = img[:,:,1:,:]-img[:,:,:-1,:]\n",
    "        d_dy = img[:,:,:,1:]-img[:,:,:,:-1]\n",
    "        # I ignore the first row or column of the image when computing the norm, in order to have vectors with matching sizes\n",
    "        # Thus, d_dx and d_dy are not strictly colocated, but that should be a good enough approximation because neighbouring pixels are correlated\n",
    "        L2 = torch.sqrt(d_dx[:,:,:,1:]**2 + d_dy[:,:,1:,:]**2 + eps)\n",
    "        TV = torch.sum(L2**self.beta) # intensity regularizer\n",
    "\n",
    "        loss = 1./H/W/(self.V**self.beta) * TV\n",
    "#         loss = 1./H/W * (torch.sum(d_dx**2) + torch.sum(d_dy**2))\n",
    "        return loss\n",
    "    \n",
    "    def forward(self,img,feature_map, feature_map_target):\n",
    "        self.content_loss = self.get_content_loss(feature_map, feature_map_target)\n",
    "        self.int_loss = self.get_int_loss(img)\n",
    "        self.tv_loss = self.get_int_loss(img)\n",
    "        self.clamp_loss = self.get_clamp_loss(img)\n",
    "        \n",
    "        self.total_loss = ( self.content_weight*self.content_loss \n",
    "                    + self.int_weight*self.int_loss \n",
    "                    + self.tv_weight*self.tv_loss \n",
    "                    + self.clamp_weight*self.clamp_loss )\n",
    "        \n",
    "        return self.total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ccdd7-0bc1-41d4-b34a-8f39c023432b",
   "metadata": {},
   "source": [
    "# 5. Training\n",
    "## 5.1. Setup training\n",
    "\n",
    "We create two instances of the `Image` class, for the *content* and *generated* images, respectively. We instantiate `SmallNet` and pass it the `content_layer`. We instantiate `Losses` and we chose weights for the regularizers. We create an optimizer and pass it the optimizable *generated* image. Here, we use L-BFGS as recommended by [Gatys et al. (2016)](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). We also define conditions of absolute and relative loss limits to stop training, and an option to jitter or not the input image during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68496dfc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1624862286526,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "68496dfc",
    "outputId": "da93fae4-880c-4cc2-c73f-313f13741162"
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [Errno 8] nodename nor servname provided, or not known>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[1;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;34m\"\"\"Connect to the host and port specified in __init__.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         self.sock = self._create_connection(\n\u001b[0m\u001b[1;32m    916\u001b[0m             (self.host,self.port), self.timeout, self.source_address)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-fc29aba20364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcontent_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://github.com/scijs/baboon-image/blob/master/baboon.png?raw=true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_im\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mplugin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tifffile'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/skimage/io/util.py\u001b[0m in \u001b[0;36mfile_or_url_context\u001b[0;34m(resource_name)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNamedTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# f must be closed before yielding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'urllib.Request'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[1;32m    543\u001b[0m                                   '_open', req)\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[1;32m   1361\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1319\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno 8] nodename nor servname provided, or not known>"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "# Images\n",
    "content_im = skimage.io.imread(\"https://github.com/scijs/baboon-image/blob/master/baboon.png?raw=true\")\n",
    "fig, ax = plt.subplots(1,1,figsize=[5,5])\n",
    "_ = plt.imshow(content_im); plt.title(\"content\"); _ = plt.axis(\"off\")\n",
    "\n",
    "img_content = Image(img=content_im, optimizable=False).to(device)\n",
    "img_gen = Image(None, optimizable=True).to(device)\n",
    "\n",
    "# SmallNet\n",
    "net = SmallNet(pretrained_net, content_layer[-1])\n",
    "\n",
    "# Losses\n",
    "loss_fn = Losses(img_content(), \n",
    "                 tv_weight=10.0, \n",
    "                 int_weight=0.1,\n",
    "                 clamp_weight=1e6)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.LBFGS(img_gen.parameters(),lr=1.0)\n",
    "abs_loss_limit = 1e-3\n",
    "rel_loss_limit = 1e-7\n",
    "\n",
    "# Jitter the input image?\n",
    "jitter = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c1cc11",
   "metadata": {
    "id": "25c1cc11"
   },
   "source": [
    "# 5.2. Actual training\n",
    "\n",
    "Here, we train the model. At each iteration we compute the feature maps of the *generated* image and compare it to the feature maps of the *content image* to compute the loss. The feature maps of the *content image* are computed only once if `jitter==False` or at each epoch otherwise. We visualize the updated *generated* image every few epochs along the current values of loss and regularizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd72045",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1354797,
     "status": "ok",
     "timestamp": 1624863642417,
     "user": {
      "displayName": "Arthur Bauville",
      "photoUrl": "",
      "userId": "13412020910432312638"
     },
     "user_tz": -540
    },
    "id": "4bd72045",
    "outputId": "b2e7b3ba-37df-4943-fede-2f2abda5f61c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=[10,10])\n",
    "\n",
    "if jitter == False:\n",
    "    fm_content = net(img_content(), content_layer)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    fm_gen = net(img_gen(), content_layer)\n",
    "    loss = loss_fn(img_gen(), fm_gen, fm_content)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "last_loss = 1e10\n",
    "frame = 0\n",
    "for i in range(1000):    \n",
    "    if jitter:\n",
    "        fm_content = net(img_content(jitter=True), content_layer)\n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    if i%1==0:\n",
    "        with torch.no_grad():\n",
    "            plt.clf()\n",
    "            plt.imshow(img_gen.postprocess())\n",
    "            plt.title(f\"epoch {i:02}, content, tv, intensity, clamp losses:\" + \n",
    "                      f\"{loss_fn.content_weight*loss_fn.content_loss:.2e}, \" + \n",
    "                      f\"{loss_fn.tv_weight*loss_fn.tv_loss:.2e}, \" +\n",
    "                      f\"{loss_fn.int_weight*loss_fn.int_loss:.2e}, \" + \n",
    "                      f\"{loss_fn.clamp_weight*loss_fn.clamp_loss:.2e}, \"+\n",
    "                      f\"total:{loss_fn.total_loss:.2e}, abs: {torch.abs(last_loss-loss_fn.total_loss):.2e}\")\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "            display(fig)\n",
    "            \n",
    "            # plt.savefig(f\"./Output/Frame{frame:05d}\")\n",
    "            # frame += 1\n",
    "\n",
    "            if loss_fn.total_loss<abs_loss_limit:\n",
    "                clear_output(wait = True)\n",
    "                print(f'success: absolute loss limit ({abs_loss_limit:.1e}) reached')\n",
    "                break\n",
    "            if torch.abs(last_loss-loss_fn.total_loss)<rel_loss_limit:\n",
    "                clear_output(wait = True)\n",
    "                print(f'stopped because relative loss limit ({rel_loss_limit:.1e})  was reached')\n",
    "                break\n",
    "\n",
    "            if loss_fn.total_loss.isnan():\n",
    "                print(f'stopped because loss is NaN')\n",
    "                break\n",
    "                \n",
    "            last_loss = loss_fn.total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4097eb-1fd8-4453-b040-5874852888d4",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605424b0-0767-458f-9660-99fa7b726e74",
   "metadata": {},
   "source": [
    "In this blogpost I implemented feature visualization and layed the foundation for my implementation of neural style transfer. My implementation of feature visualization follows the recommandation of [Mahendran and Vedaldi (2016)](https://arxiv.org/abs/1512.02017). I found that the trickiest part of the implementation, and where I spent the most time debugging was in the feature map extraction. At this point it is important to clone and detach feature maps appropriately. Failing to do so may break backpropagation, or worse, backpropagation can go on but with the wrong feature maps which result in reasonnable but underwhelming output. This was the hardest part of the work. See you in the next part of this series, where I'll explore style extraction!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Blog01_Feature_visualization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00d07475ef754aa9bd2ee65d2297e211": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0e53203d08df409d879e56596de0da32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "401dc2fa55cc44cbb7afe2558e1c50c6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4352c9184ab94a81904134f8f0b14af0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_401dc2fa55cc44cbb7afe2558e1c50c6",
      "placeholder": "​",
      "style": "IPY_MODEL_d15b9e21a5f94415b6f5b5a14d3e4c8a",
      "value": " 528M/528M [01:58&lt;00:00, 4.69MB/s]"
     }
    },
    "795cf662825d4b69b89fdd2c13659236": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cadfc3f543c47afb5feecdfbd9d3228": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e53203d08df409d879e56596de0da32",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_00d07475ef754aa9bd2ee65d2297e211",
      "value": 553433881
     }
    },
    "c4ae12751d764e319493100bd1b263c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7cadfc3f543c47afb5feecdfbd9d3228",
       "IPY_MODEL_4352c9184ab94a81904134f8f0b14af0"
      ],
      "layout": "IPY_MODEL_795cf662825d4b69b89fdd2c13659236"
     }
    },
    "d15b9e21a5f94415b6f5b5a14d3e4c8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"colab":{"name":"03_Style_transfer.ipynb","provenance":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"18e223a369f0495e9b14f4af360a94ab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5a80f98097a34cbb9d7f29d18652dc7f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b79574a0ee434120ae008cc0763f6173","IPY_MODEL_53b803edd0b84c429370d942ec6f385d"]}},"5a80f98097a34cbb9d7f29d18652dc7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b79574a0ee434120ae008cc0763f6173":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4eba80f2f79e4217947a5bc2f6bb5bba","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da32cf1d3f3746e696711dfe9914e617"}},"53b803edd0b84c429370d942ec6f385d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_93d5928c438d4224a60ad301c774e974","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:07&lt;00:00, 76.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8fce163e6a144f9af3763efa628b3f9"}},"4eba80f2f79e4217947a5bc2f6bb5bba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"da32cf1d3f3746e696711dfe9914e617":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"93d5928c438d4224a60ad301c774e974":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c8fce163e6a144f9af3763efa628b3f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"19d7ec73"},"source":["# Compute the style of an image\n","\n","Based on the paper by Gatis et al. 2016 ([ref](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)).\n"],"id":"19d7ec73"},{"cell_type":"code","metadata":{"id":"9cc9705f","executionInfo":{"status":"error","timestamp":1623311453421,"user_tz":-540,"elapsed":4300,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}},"outputId":"396ce8fb-5166-4c00-9007-bc73d2d8f03f","colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["# Imports\n","import torch\n","import torchvision\n","from torch import nn\n","import skimage\n","from skimage import transform\n","from im_func import show_image, timer\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython.display import display, clear_output"],"id":"9cc9705f","execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-59da0073969f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mim_func\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'im_func'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"cb90a1fa"},"source":["# Get images"],"id":"cb90a1fa"},{"cell_type":"code","metadata":{"id":"7b3175e6","executionInfo":{"status":"aborted","timestamp":1623311453418,"user_tz":-540,"elapsed":4,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}}},"source":["# content_im = skimage.data.coffee()\n","content_im = skimage.io.imread(\"./Data/Cecile.jpg\")\n","style_im = skimage.io.imread(\"./Data/PICASSO_WOMAN.jpeg\")\n","# im = skimage.transform.rescale(im, 0.25, anti_aliasing=False, multichannel=True)\n","\n","fig, ax = plt.subplots(1,2,figsize=[10,5])\n","plt.sca(ax[0])\n","_ = show_image(content_im,'content')\n","plt.sca(ax[1])\n","_ = show_image(style_im,'style')\n","# im.max()\n","# im = im/255.\n","# im = im.astype(np.float32)"],"id":"7b3175e6","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"60f8bab6"},"source":["# Pre/post processing of image\n","\n","Normalization, resizing etc... to correspond to the input format of images trained by t"],"id":"60f8bab6"},{"cell_type":"code","metadata":{"id":"b7a08afd","executionInfo":{"status":"aborted","timestamp":1623311453420,"user_tz":-540,"elapsed":6,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}}},"source":["# From D2L\n","im_shape = (150, 225)\n","rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n","rgb_std = torch.tensor([0.229, 0.224, 0.225])\n","\n","def preprocess(img, image_shape):\n","    transforms = torchvision.transforms.Compose([\n","        torchvision.transforms.ToPILImage(),\n","        torchvision.transforms.Resize(image_shape),\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)\n","    ])\n","    return transforms(img).unsqueeze(0)\n","\n","def postprocess(img):\n","    img = img[0].to(rgb_std.device)\n","    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n","    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"],"id":"b7a08afd","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7c65b88c"},"source":["# Get a pretrained model"],"id":"7c65b88c"},{"cell_type":"code","metadata":{"id":"739d2edd","executionInfo":{"status":"ok","timestamp":1623311463252,"user_tz":-540,"elapsed":9837,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}},"outputId":"ac237a0c-84f7-4075-fb29-0dba0e649fed","colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["18e223a369f0495e9b14f4af360a94ab","5a80f98097a34cbb9d7f29d18652dc7f","b79574a0ee434120ae008cc0763f6173","53b803edd0b84c429370d942ec6f385d","4eba80f2f79e4217947a5bc2f6bb5bba","da32cf1d3f3746e696711dfe9914e617","93d5928c438d4224a60ad301c774e974","c8fce163e6a144f9af3763efa628b3f9"]}},"source":["pretrained_net = torchvision.models.vgg19(pretrained=True)"],"id":"739d2edd","execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18e223a369f0495e9b14f4af360a94ab","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f3fd4388","executionInfo":{"status":"ok","timestamp":1623311463253,"user_tz":-540,"elapsed":9,"user":{"displayName":"Arthur Bauville","photoUrl":"","userId":"13412020910432312638"}},"outputId":"80856f81-a9c1-4c40-833c-277e0e4b2a62","colab":{"base_uri":"https://localhost:8080/"}},"source":["pretrained_net.features[0]"],"id":"f3fd4388","execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"f20b478c"},"source":["# Defined the image generator network and a function to make a partial pass through the network"],"id":"f20b478c"},{"cell_type":"code","metadata":{"id":"84b68027"},"source":["class ImageGenerator(nn.Module):\n","    def __init__(self,shape):\n","        super(ImageGenerator,self).__init__()\n","        self.im = nn.Parameter(torch.rand(1, 3, *shape))\n","    def forward(self):\n","        return self.im"],"id":"84b68027","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7e939364"},"source":["# Apply layer by layer mods\n","def partial_forward(input_im,n_layer):\n","    X = input_im\n","    for il in range(n_layer):\n","        X = pretrained_net.features[il](X)\n","    return X\n","\n","def visu_im_rep(X, ncol=5):\n","    nrow = int(np.ceil(n_channels/ncol))\n","    fig, ax = plt.subplots(nrow,ncol,figsize=[15,24],tight_layout=True)\n","    with torch.no_grad():\n","        for ic in range(n_channels):\n","            plt.sca(ax[np.unravel_index(ic,(nrow,ncol))])\n","            show_image(X[0,ic,:,:])"],"id":"7e939364","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77e74a3c"},"source":["`x` is the representation of our input image by the given layer. This representation is composed of many channels, each the result of a specific convolution that has been optimized through training to extract specific useful features. \n","Now, we instantiate the simple ImageGenerator. `im_gen` parameters' (`list(im_gen.parameters())[0]`) contains a tensor initialized with random noise. "],"id":"77e74a3c"},{"cell_type":"code","metadata":{"id":"b503244d","outputId":"6b2fa60e-3ed5-491f-e575-71fc212b18d8"},"source":["# len(pretrained_net.features)\n","pretrained_net.named_parameters"],"id":"b503244d","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method Module.named_parameters of VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (17): ReLU(inplace=True)\n","    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): ReLU(inplace=True)\n","    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (31): ReLU(inplace=True)\n","    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (33): ReLU(inplace=True)\n","    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (35): ReLU(inplace=True)\n","    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=1000, bias=True)\n","  )\n",")>"]},"metadata":{"tags":[]},"execution_count":102}]},{"cell_type":"markdown","metadata":{"id":"f2352c0a"},"source":["# Compute style"],"id":"f2352c0a"},{"cell_type":"code","metadata":{"id":"a26cdd7f"},"source":["\n","\n","def get_feature_maps(image, layer_list):\n","#      return [pretrained_net.features[il](image) for il in range (layer_list[-1]+1) if (il in layer_list) ]\n","    a = image \n","    feature_maps = []\n","    for il in range(layer_list[-1]+1):\n","#         a = pretrained_net.features[il](a)\n","        a = net[il](a)\n","        if (il in layer_list):\n","            feature_maps.append(a)\n","    return feature_maps\n","\n","def get_gram(feature_maps):\n","    gram_matrices = []\n","    for a in feature_maps:\n","        fac = 1./(2.*a.shape[1]*a.shape[2]*a.shape[3])\n","        a = a.reshape(a.shape[1],-1) \n","        gram_matrices.append(torch.matmul(a,a.T)*fac)\n","\n","    return gram_matrices\n","\n","def get_style_loss(gram_style_image, gram_generated_image):\n","    loss = 0\n","    for A, G in zip(gram_style_image,gram_generated_image):\n","        loss += torch.sum((A-G)**2)\n","    return loss\n","\n","def get_content_loss(feature_map_content, feature_map_gen):\n","    loss = 0\n","    for i in range(len(feature_map_content)):\n","         loss += torch.mean((feature_map_content[i]-feature_map_gen[i])**2)\n","    return loss\n"],"id":"a26cdd7f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"36d2d785"},"source":["def get_TV_loss(im):\n","    # y: filtered representation of content image on the given layer of the NN\n","    # y_hat: same as y for generated image\n","    # im: the generated image\n","    \n","    beta = 1. # Can be between 1 and 2\n","    H = im.shape[2]\n","    W = im.shape[3]\n","    C = im.shape[1]\n","    sigma = torch.sqrt(torch.sum(im**2))/H/W/C\n","    B = 1. # because images are standardized\n","    a = 0.01 # 1%\n","    Lambda_b = sigma**beta / (H*W*(a*B)**beta)# There is a better definition in the paper\n","    \n","    # total variation\n","    d_dx = im[:,:,1:,:]-im[:,:,:-1,:]\n","    d_dy = im[:,:,:,1:]-im[:,:,:,:-1]\n","    d_dx = .5*(d_dx[:,:,:,1:]+d_dx[:,:,:,:-1])\n","    d_dy = .5*(d_dy[:,:,1:,:]+d_dy[:,:,:-1,:])\n","    TV = torch.sum((d_dx**2+d_dy**2)**(beta/2.))\n","#     print(f\"MSE: {loss:.2e}, TV: {TV:.2e}, $\\\\lambda$ TV: {Lambda_b*TV:.2e}\")\n","    loss = Lambda_b*TV\n","    return loss"],"id":"36d2d785","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62645101"},"source":["# Initialize GPU if available"],"id":"62645101"},{"cell_type":"code","metadata":{"id":"0c5dee88","outputId":"fa2e1e92-80e7-4cc8-f8ea-d215ab8501cc"},"source":["# setting device on GPU if available, else CPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device:', device)\n","print()\n","\n","#Additional Info when using cuda\n","if device.type == 'cuda':\n","    print(torch.cuda.get_device_name(0))\n","    print('Memory Usage:')\n","    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n","    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"],"id":"0c5dee88","execution_count":null,"outputs":[{"output_type":"stream","text":["Using device: cpu\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"34ce7acb"},"source":["# Train"],"id":"34ce7acb"},{"cell_type":"code","metadata":{"id":"e35d4042"},"source":["im_gen = ImageGenerator(im_shape).to(device)\n","im_style = preprocess(style_im, im_shape).to(device)\n","im_content = preprocess(content_im, im_shape).to(device)\n","\n","optim = torch.optim.LBFGS(im_gen.parameters())\n","# optim = torch.optim.Adam(im_gen.parameters())\n","# Stopping criterion\n","abs_loss_limit = 1e-10\n","rel_loss_limit = 1e-12\n","\n","\n","# layer_list = [8,17, 26, 35]\n","# layer_list = [0,1,2,3,4,5]\n","style_layer_list = [8,17, 26, 35]\n","content_layer = [8]\n","\n","net = nn.Sequential(*[\n","    pretrained_net.features[i]\n","    for i in range(max(content_layer + style_layer_list) + 1)]).to(device)\n","\n","with torch.no_grad():\n","    fm_style = get_feature_maps(im_style, style_layer_list)\n","    fm_content = get_feature_maps(im_content, content_layer)\n","    gram_style = get_gram(fm_style)\n","content_weight = 0.06\n","style_weight = 1.0\n","tv_weight = 1.0"],"id":"e35d4042","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8719d89"},"source":["# train\n","fig, ax = plt.subplots(1,1,figsize=[10,10])\n","\n","def closure_small():\n","\n","    im_generated = im_gen()\n","#     fm_style = get_feature_maps(im_style.detach(), style_layer_list)\n","#     gram_style = get_gram(fm_style)\n","    fm_gen_style = get_feature_maps(im_generated, style_layer_list)\n","    fm_gen_content = get_feature_maps(im_generated, content_layer) # having to go through the network again is a bit ugly\n","    gram_gen = get_gram(fm_gen_style)\n","    content_loss = get_content_loss(fm_content, fm_gen_content)\n","    style_loss = get_style_loss(gram_style,gram_gen)\n","    tv_loss = get_TV_loss(im_generated)\n","    \n","    return content_loss, style_loss, tv_loss\n","def closure():\n","    optim.zero_grad()\n","    content_loss, style_loss, tv_loss = closure_small()\n","    loss = content_weight*content_loss + style_weight*style_loss + tv_weight*tv_loss\n","    print(f\"content, style, tv losses: {content_weight*content_loss:.2e}, {style_weight*style_loss:.2e}, {tv_weight*tv_loss:.2e}\")\n","    loss.backward()\n","    return loss\n","\n","last_loss = 1e10\n","for i in range(100):    \n","#     im_generated = im_gen()\n","    optim.step(closure)\n","    if i%1==0:\n","        with torch.no_grad():\n","            content_loss, style_loss, tv_loss = closure_small()\n","            loss = content_weight*content_loss + style_weight*style_loss + tv_weight*tv_loss\n","            imnew = postprocess(im_gen()[0])\n","            ax.cla()\n","            plt.imshow(imnew)\n","            plt.title(f\"epoch {i:02d}, content, style, tv losses: {content_weight*content_loss:.2e}, {style_weight*style_loss:.2e}, {tv_weight*tv_loss:.2e}\")\n","            clear_output(wait = True)\n","            display(fig)\n","\n","            if loss<abs_loss_limit:\n","                clear_output(wait = True)\n","                print(f'success: absolute loss limit ({abs_loss_limit:.1e}) reached')\n","                break\n","            if torch.abs(last_loss-loss)<rel_loss_limit:\n","                clear_output(wait = True)\n","                print(f'stopped because relative loss limit ({rel_loss_limit:.1e})  was reached')\n","                break\n","                \n","            last_loss = loss"],"id":"d8719d89","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"68341036"},"source":[""],"id":"68341036","execution_count":null,"outputs":[]}]}
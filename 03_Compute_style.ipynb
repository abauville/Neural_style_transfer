{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45dc8adf",
   "metadata": {},
   "source": [
    "# Compute the style of an image\n",
    "\n",
    "Based on the paper by Gatis et al. 2016 ([ref](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "206ebaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import skimage\n",
    "from skimage import transform\n",
    "from im_func import show_image, timer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28e139",
   "metadata": {},
   "source": [
    "# Get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0517fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = skimage.data.coffee()\n",
    "# im = skimage.transform.rescale(im, 0.25, anti_aliasing=False, multichannel=True)\n",
    "_ = show_image(im)\n",
    "# im.max()\n",
    "# im = im/255.\n",
    "# im = im.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b6893",
   "metadata": {},
   "source": [
    "# Pre/post processing of image\n",
    "\n",
    "Normalization, resizing etc... to correspond to the input format of images trained by t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc872e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From D2L\n",
    "im_shape = (150, 225)\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToPILImage(),\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)\n",
    "    ])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "#     img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d831b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-proc to image\n",
    "im_new = preprocess(im,im_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0049843c",
   "metadata": {},
   "source": [
    "# Get a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c2a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_net = torchvision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea0a8b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_net.features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ddbaf",
   "metadata": {},
   "source": [
    "# Defined the image generator network and a function to make a partial pass through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad201a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super(ImageGenerator,self).__init__()\n",
    "        self.im = nn.Parameter(torch.rand(1, 3, *shape))\n",
    "    def forward(self):\n",
    "        return self.im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f68978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply layer by layer mods\n",
    "def partial_forward(input_im,n_layer):\n",
    "    X = input_im\n",
    "    for il in range(n_layer):\n",
    "        X = pretrained_net.features[il](X)\n",
    "    return X\n",
    "\n",
    "def visu_im_rep(X, ncol=5):\n",
    "    nrow = int(np.ceil(n_channels/ncol))\n",
    "    fig, ax = plt.subplots(nrow,ncol,figsize=[15,24],tight_layout=True)\n",
    "    with torch.no_grad():\n",
    "        for ic in range(n_channels):\n",
    "            plt.sca(ax[np.unravel_index(ic,(nrow,ncol))])\n",
    "            show_image(X[0,ic,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ab632",
   "metadata": {},
   "source": [
    "`x` is the representation of our input image by the given layer. This representation is composed of many channels, each the result of a specific convolution that has been optimized through training to extract specific useful features. \n",
    "Now, we instantiate the simple ImageGenerator. `im_gen` parameters' (`list(im_gen.parameters())[0]`) contains a tensor initialized with random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9355cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.named_parameters of VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(pretrained_net.features)\n",
    "pretrained_net.named_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b66a7e",
   "metadata": {},
   "source": [
    "# Compute style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "399e983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def partial(a,x,n_layer):\n",
    "#     for il in range(n_layer):\n",
    "#         a = pretrained_net.features[il](a)\n",
    "#         x = pretrained_net.features[il](x)\n",
    "    \n",
    "#         fac = 1./(2.*a.shape[1]*a.shape[2]*a.shape[3])\n",
    "#         aa = a.reshape(a.shape[1],-1) \n",
    "#         xx = x.reshape(x.shape[1],-1) \n",
    "    \n",
    "#         A = torch.matmul(aa,aa.T)*fac # Gram matrix\n",
    "#         G = torch.matmul(xx,xx.T)*fac\n",
    "\n",
    "#     return torch.sum(((A-G))**2)\n",
    "# #     l0 += torch.sum(((A-G))**2)\n",
    "\n",
    "\n",
    "# def partial_forward_style_loss(style_im, gen_im, n_layer):\n",
    "#     a = style_im.detach()\n",
    "#     x = gen_im\n",
    "    \n",
    "#     loss = sum([partial(a,x,l) for l in range(2,n_layer+1,2)])\n",
    "    \n",
    "# #     print(loss)\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3c762f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def partial_forward_style_loss(style_im, gen_im, n_layer):\n",
    "    a = style_im.detach()\n",
    "    x = gen_im\n",
    "    loss = []\n",
    "\n",
    "    styles = []\n",
    "    gen_styles = []\n",
    "    for il in range(n_layer):\n",
    "        a = pretrained_net.features[il](a)\n",
    "        x = pretrained_net.features[il](x)\n",
    "        gen_styles.append(x)\n",
    "        styles.append(a)\n",
    "    \n",
    "    gram_styles = []\n",
    "    gram_gen = []\n",
    "    for a, x in zip(styles,gen_styles):\n",
    "        fac = 1./(2.*a.shape[1]*a.shape[2]*a.shape[3])\n",
    "        aa = a.reshape(a.shape[1],-1) \n",
    "        xx = x.reshape(x.shape[1],-1) \n",
    "\n",
    "        A = torch.matmul(aa,aa.T)*fac # Gram matrix\n",
    "        G = torch.matmul(xx,xx.T)*fac\n",
    "\n",
    "        gram_styles.append(A)\n",
    "        gram_gen.append(G)\n",
    "        \n",
    "    loss = 0    \n",
    "    for A, G in zip(gram_styles,gram_gen):\n",
    "        loss += torch.sum((A-G)**2)\n",
    "    \n",
    "    \n",
    "#     loss = sum(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1195fd9d",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "aead903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_gen = ImageGenerator(im_shape)\n",
    "# current_im = im_gen()\n",
    "# preprocess(current_im,im_shape).shape\n",
    "layer = 8\n",
    "im_style = preprocess(im,im_shape)\n",
    "# target_rep = partial_forward(preprocess(im,im_shape), layer)\n",
    "# loss_fn = MSE_TV_loss\n",
    "optim = torch.optim.LBFGS(im_gen.parameters())\n",
    "# optim = torch.optim.Adam(im_gen.parameters())\n",
    "# Stopping criterion\n",
    "abs_loss_limit = 1e-10\n",
    "rel_loss_limit = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ae8a22e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_generated = im_gen()\n",
    "loss = partial_forward_style_loss(im_style, im_generated, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159ef76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4982e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "584c1a39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 8400]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-35f3a6e5a3aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     im_generated = im_gen()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-35f3a6e5a3aa>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial_forward_style_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_generated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 8400]], which is output 0 of ViewBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "fig, ax = plt.subplots(1,1,figsize=[10,10])\n",
    "\n",
    "def closure():\n",
    "#     if torch.is_grad_enabled():\n",
    "    im_generated = im_gen()\n",
    "    optim.zero_grad()\n",
    "    loss = partial_forward_style_loss(im_style, im_generated, layer)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "last_loss = 1e10\n",
    "for i in range(100):    \n",
    "#     im_generated = im_gen()\n",
    "    optim.step(closure)\n",
    "    if i%1==0:\n",
    "        with torch.no_grad():\n",
    "            im_generated = im_gen()\n",
    "            loss = partial_forward_style_loss(im_style, im_generated, layer)\n",
    "            \n",
    "#             print(f\"epoch {i:02d}, loss: {loss:.10e}\")\n",
    "            \n",
    "            \n",
    "            imnew = postprocess(im_generated[0])\n",
    "            ax.cla()\n",
    "            plt.imshow(imnew)\n",
    "            plt.title(f\"epoch {i:02d}, loss: {loss:.10e}\")\n",
    "            clear_output(wait = True)\n",
    "            display(fig)\n",
    "\n",
    "            if loss<abs_loss_limit:\n",
    "                clear_output(wait = True)\n",
    "                print(f'success: absolute loss limit ({abs_loss_limit:.1e}) reached')\n",
    "                break\n",
    "            if torch.abs(last_loss-loss)<rel_loss_limit:\n",
    "                clear_output(wait = True)\n",
    "                print(f'stopped because relative loss limit ({rel_loss_limit:.1e})  was reached')\n",
    "                break\n",
    "                \n",
    "            last_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb29ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50bb15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
